# Temporal QnA Agent with MCP and Azure OpenAI

Question & Answer (Q&A) system that uses **Temporal** for workflow orchestration, **Model Context Protocol (MCP)** for semantic document search, and **Azure OpenAI** with the **OpenAI Agents SDK** for intelligent response generation.

## ğŸ—ï¸ Architecture

The project consists of:

- **Temporal Workflows**: Durable and resilient orchestration of question and answer flow
- **MCP Server**: Server that exposes semantic search tools via Model Context Protocol
- **Azure OpenAI**: LLM (GPT-4o) for response generation and embeddings for vector search
- **OpenAI Agents SDK**: Framework for creating intelligent agents with tools
- **FastAPI**: REST API for workflow interaction
- **Streamlit**: Web Interface for end users

## ğŸ“‹ Features

- âœ… Workflow orchestration with Temporal
- âœ… Semantic document search using embeddings
- âœ… Agents with tool access (MCP)
- âœ… REST API for integration
- âœ… Interactive Web Interface
- âœ… Persistent conversation history
- âœ… Asynchronous and scalable processing

## ğŸš€ Prerequisites

- **Python 3.10+**
- **Temporal Server** (local via Docker or remote)
- **Azure OpenAI** (endpoint and API keys)
- **Docker** (optional, to run Temporal locally)

## ğŸ“¦ Installation

### 1. Clone the repository

```bash
git clone https://github.com/your-username/temporal-qna-agent.git
cd temporal-qna-agent
```

### 2. Create virtual environment and install dependencies

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### 3. Configure environment variables

```bash
cp .env.example .env
# Edit the .env file with your Azure OpenAI credentials
```

### 4. Start the Temporal Server (if local)

```bash
# Using Docker Compose (see section below)
docker-compose up -d temporal
```

Or via [Temporal CLI](https://docs.temporal.io/cli):

```bash
temporal server start-dev
```

### 5. Generate search index embeddings

```bash
python database/utils.py
```

## ğŸ¯ How to Use

### Run all components

#### 1. Worker (Terminal 1)
```bash
python worker.py
```

#### 2. FastAPI API (Terminal 2)
```bash
python api/main.py
# Or using uvicorn:
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
```

#### 3. Streamlit Frontend (Terminal 3)
```bash
streamlit run frontend/app.py
```

#### 4. Access the application
- Frontend: http://localhost:8501
- API Docs: http://localhost:8000/docs

### Use via API

```bash
# Start a workflow
curl -X POST http://localhost:8000/workflows/start \
  -H "Content-Type: application/json" \
  -d '{"workflow_id": "qna-001"}'

# Send a question
curl -X POST http://localhost:8000/workflows/qna-001/prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt": "What are the best Python libraries for APIs?"}'

# Get history
curl http://localhost:8000/workflows/qna-001/history
```

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ activities/          # Temporal Activities (MCP search)
â”‚   â””â”€â”€ activities.py
â”œâ”€â”€ api/                 # FastAPI REST API
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ database/            # Document index and embeddings
â”‚   â”œâ”€â”€ index.json
â”‚   â”œâ”€â”€ search_index.json
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ frontend/            # Streamlit Interface
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ tools/               # Utilities (LLM client)
â”‚   â””â”€â”€ llm_client.py
â”œâ”€â”€ workflows/           # Temporal Workflows
â”‚   â””â”€â”€ workflow.py
â”œâ”€â”€ connection.py        # Standalone Temporal client
â”œâ”€â”€ mcp_server.py        # MCP Server for search
â”œâ”€â”€ worker.py            # Temporal Worker
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸ³ Docker Compose

```bash
# Start Temporal + PostgreSQL
docker-compose up -d

# Stop services
docker-compose down
```

## ğŸ”§ Configuration

All configurations are done via environment variables in the `.env` file:

- `AZURE_API_BASE`: Azure OpenAI endpoint
- `AZURE_API_KEY`: API key
- `AZURE_DEPLOYMENT`: Deployment name (e.g., gpt-4o)
- `AZURE_EMBEDDINGS_*`: Embeddings configurations
- `TEMPORAL_ADDRESS`: Temporal server address
- `TEMPORAL_TASK_QUEUE`: Task queue

## ğŸ§ª Testing the Project

1. Make sure the Temporal Server is running
2. Run the worker: `python worker.py`
3. In another terminal, run the test client: `python connection.py`
4. Or use the API/Frontend as described in the "How to Use" section

## ğŸ“š Additional Documentation

- [Temporal Docs](https://docs.temporal.io/)
- [Model Context Protocol](https://modelcontextprotocol.io/)
- [OpenAI Agents SDK](https://github.com/openai/openai-agents-sdk)
- [Azure OpenAI](https://learn.microsoft.com/azure/ai-services/openai/)

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the project
2. Create a branch for your feature (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is under the MIT license. See the [LICENSE](LICENSE) file for more details.

## ğŸ‘¤ Author

Your Name - [@your_github](https://github.com/your-username)